runner:
  n_epochs: 150
  total_steps: 474450
  gradient_clipping: 10.0
  gradient_accumulate_steps: 1
 
  log_step: 5
  save_step: 1000
  max_keep: 5

optimizer:
  lr: 1.e-5
  betas: [0.9, 0.999]
  eps: 1.e-8
  weight_decay: 0

datarc:
  num_workers: 8
  train_batch_size: 4
  max_timestep: -320
  sets: ['/home/nervjack2/libri-with-cluster/csv/libri-360-np-stage1.csv']

prune:
  # Prune ${num_layers} heads in each step
  # TARGET
  # 1. by_layer, prune 1 head per layer in each prune-retrain step
  # 2. by_whole, prune 12 heads in total
  # Score
  # 1. data-driven
  # 2. l1 
  target: by_layer
  metric: l1
  total_steps: 11 #total pruning steps
  num_heads_each_step: 12 # only for by_whole
  interval: [0, 25000, 50000, 75000, 100000, 125000, 165000, 205000, 245000, 285000, 325000, 365000, 405000]
  #interval: [1, 6, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
  save_step: 1 # in terms of prune steps
  warm_up: 1000
  #warm_up: 5
  data_ratio: 0.25
  #data_ratio: 0.001
  update_score_step: 1 # update head scores per n steps
  normalize_by_layer: 2 # normalize data-drive grad by k-norm 
  test_reverse: False # test reversed head score by verifying finetune loss
