runner:
  n_epochs: -1
  total_steps: 335000
  gradient_clipping: 10.0
  gradient_accumulate_steps: 1
  fp16: false #true
  log_step: 1

task:
  _name: hubert_pretraining
  data: /mnt/data/ycevan/datasets/LibriSpeech/pseudolabel_manifest_debug
  label_dir: /mnt/data/ycevan/datasets/LibriSpeech/labels/speculate_debug
  labels: ['km']
  label_rate: 50
  sample_rate: 16000
  max_sample_size: 250000
  min_sample_size: 32000
  pad_audio: false
  random_crop: false #true
  normalize: false # must be consistent with extractor
  shuffle: false

pretrain_expert:
  datarc:
    num_workers: 6
    train_batch_size: 1

optimizer:
  lr: 5.e-5
  betas: [0.9, 0.98]
  eps: 1e-06
  weight_decay: 0.01

lr_scheduler:
  warmup_updates: 32000

prune:
  # Prune ${num_layers} heads in each step
  # TARGET
  # 1. by_layer, prune 1 head per layer in each prune-retrain step
  # 2. by_whole, prune 12 heads in total
  # Score
  # 1. data-driven
  # 2. l1 
  target: by_layer
  metric: l1
  total_steps: 11 #total pruning steps
  num_heads_each_step: 12 # only for by_whole
  interval: [0, 25000, 50000, 75000, 100000, 125000, 165000, 205000, 245000, 285000, 325000]
  warm_up: 10000
  data_ratio: 0.25 # only used when metric is data-driven
  normalize_by_layer: 2 # normalize data-drive grad by k-norm 
