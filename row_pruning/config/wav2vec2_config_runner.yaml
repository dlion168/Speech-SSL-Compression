common:
  # fp16: true
  log_format: json
  log_interval: 200

checkpoint:
  save_interval_updates: 25000
  keep_interval_updates: 1
  no_epoch_checkpoints: true

task:
  _name: audio_pretraining
  data: /mnt/data/ycevan/datasets/Librispeech_manifest
  max_sample_size: 250000
  min_sample_size: 32000
  normalize: false

# dataset:
#   num_workers: 6
#   max_tokens: 1400000
#   skip_invalid_size_inputs_valid_test: true

# distributed_training:
#   distributed_world_size: 64
#   ddp_backend: legacy_ddp

# optimization:
#   max_update: 400000

pretrain_expert:
  datarc:
    num_workers: 6
    train_batch_size: 12
  
runner:
  n_epochs: 18
  total_steps: -1
  # gradient_clipping: 10.0
  gradient_accumulate_steps: 1

  log_step: 200
  save_step: 50000
  max_keep: 5
  fp16: true
  
optimizer:
  lr: 0.0005
  betas: (0.9,0.98)
  eps: 1e-06
  weight_decay: 0.01

lr_scheduler:
  warmup_updates: 32000

prune:
  total_steps: 20 # total pruning steps
  num_rows_each_step: 128
  interval: [0, 25000, 50000, 75000, 100000, 125000, 150000, 175000, 200000, 
              225000, 250000, 275000, 300000, 325000, 350000, 375000, 400000, 425000, 450000, 475000]
  warm_up: 1000
