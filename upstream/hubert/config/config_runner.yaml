common:
  log_format: json
  seed: 1337
  tensorboard_logdir: tblog

checkpoint:
  save_interval_updates: 25000
  keep_interval_updates: 1
  no_epoch_checkpoints: true

task:
  _name: hubert_pretraining
  data: /mnt/data/ycevan/datasets/LibriSpeech/pseudolabel_manifest
  label_dir: /mnt/data/ycevan/datasets/LibriSpeech/labels/960h-hubert-speculte-stg2-target
  labels: ['km']
  label_rate: ${model.label_rate}
  sample_rate: 16000
  max_sample_size: 250000
  min_sample_size: 32000
  pad_audio: false
  random_crop: true
  normalize: false # must be consistent with extractor

# dataset:
#   max_tokens: 1400000
#   skip_invalid_size_inputs_valid_test: true
#   validate_interval: 5
#   validate_interval_updates: 10000

pretrain_expert:
  datarc:
    num_workers: 6
    train_batch_size: 12
    max_timestep: -200 # Max length for audio feature (0 for no restriction, negative value to set minimum timestep)
    libri_root: '/work/a129195789/LibriSpeech/' # If raw libri data is provided, use on-the-fly feature extraction, else use the pre-extracted features under `file_path`
    file_path: 'data/len_for_bucket' # Pre-extracted features path. When using on-the-fly feature extraction, this is used to provide length for bucketing.
    sets: ['train-clean-100', 'train-clean-360', 'train-other-500'] # can be the subset of ['train-clean-100', 'train-clean-360', 'train-other-500']
  
runner:
  n_epochs: 18
  total_steps: -1
  gradient_clipping: 10.0
  gradient_accumulate_steps: 1

  log_step: 200
  save_step: 50000
  max_keep: 5
  fp16: true

optimizer:
  lr: 0.0005
  betas: (0.9,0.98)
  eps: 1e-06
  weight_decay: 0.01

lr_scheduler:
  warmup_updates: 32000