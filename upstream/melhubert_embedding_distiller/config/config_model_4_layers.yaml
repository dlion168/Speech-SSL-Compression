student:
  # Input feature dimension 
  feat_emb_dim: 80
  # Convolutional relative positional encoding
  pos_emb_type: conv                                  # Options: ["conv"]. The original implementation in HuBERT is "conv".
  pos_conv_depth: 1
  conv_pos: 128
  conv_pos_groups: 16

  learnable_mask_emb: False
  mask_before_proj: True

  # Transformer encoder
  encoder_layers: 4
  encoder_embed_dim: 768
  encoder_ffn_embed_dim: 3072
  encoder_attention_heads: 12
  activation_fn: gelu
  layer_norm_first: false
  attention_type: original

  # Dropout
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.1
  encoder_layerdrop: 0.0

  # Output
  final_dim: 768
  out_layer_type: expand-last

  # Task & loss
  n_tasks: 2
  task_emb_type: expand-last
  loss_type: l1
  feat_pen_loss: 0.0
  cosine_loss: 1.0  # cosine similarity loss
  pred_layer_id: [4, 12]

  # Initialization
  initial_from_teacher: false

teacher:
  # Input feature dimension 
  feat_emb_dim: 80

  # Positional embedding type
  pos_emb_type: conv                                  # Options: ["conv"]. The original implementation in HuBERT is "conv".
  pos_conv_depth: 1
  conv_pos: 128
  conv_pos_groups: 16

  # Transformer encoder
  encoder_layers: 12
  encoder_embed_dim: 768
  encoder_ffn_embed_dim: 3072
  encoder_attention_heads: 12
  activation_fn: gelu
  layer_norm_first: False
  attention_type: original
  
  # Output dimension 
  num_cluster: 512 

  # Criterion 
  pred_masked_weight: 1.0
  pred_nomask_weight: 0.0

  skip_masked: False
  skip_nomask: True

  # Masking config
  mask_prob: 0.70
  mask_length: 5
  mask_selection: 'static'
  mask_other: 0.0
  no_mask_overlap: False
  mask_min_space: 1

  learnable_mask_emb: False
  mask_before_proj: True
  
  # Dropout
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.1
  encoder_layerdrop: 0.0

  init_path: /mnt/data/ycevan/Speech-SSL-Compression/upstream/melhubert/melhubert-20ms-stage2-libri960-100epochs.ckpt

task:
  sequence_length: 750