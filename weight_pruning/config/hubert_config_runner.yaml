runner:
  n_epochs: -1 
  total_steps: 1000000
  gradient_clipping: 10.0
  gradient_accumulate_steps: 1
  
  log_step: 500

# runner:
#   n_epochs: 18
#   total_steps: -1
#   gradient_clipping: 10.0
#   gradient_accumulate_steps: 1

#   log_step: 200
#   save_step: 50000
#   max_keep: 5
#   fp16: true

task:
  _name: hubert_pretraining
  data: /mnt/data/ycevan/datasets/LibriSpeech/pseudolabel_manifest
  label_dir: /mnt/data/ycevan/datasets/LibriSpeech/labels/960h-hubert-speculte-stg2-target
  labels: ['km']
  label_rate: 50
  sample_rate: 16000
  max_sample_size: 250000
  min_sample_size: 32000
  pad_audio: false
  random_crop: true
  normalize: false # must be consistent with extractor

pretrain_expert:
  datarc:
    num_workers: 6
    train_batch_size: 12

optimizer:
  lr: 0.0005
  betas: (0.9,0.98)
  eps: 1e-06
  weight_decay: 0.01

lr_scheduler:
  warmup_updates: 32000

prune:
  sparsity: [.2,.3,.4,.5,.55,.6,.65,.675,.7,.71,.72,.73,.74,.75,.76,.77,.78,.79,.8,.81,.82,.83,.84,.85,.86,.87,.88,.89,.9, .91, .915, .92, .925, .93, .935, .94, .945, .95]
  # warnup, and period are steps
  warnup: 25000 # before pruning
  period: 25000 # between two pruning
  n_iters: 38 # times of pruning
  pruning_condition: "converge" # normal, converge
  converge_loss_tolerance: 0.001 # only work in converge mode, tolerant how much performance drop
  average_length: 15000 # only work in converge mode
  smooth_factor: 0.9998
  strategy: "L1Unstructured"
