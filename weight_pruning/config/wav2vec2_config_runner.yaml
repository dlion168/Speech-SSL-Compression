runner:
  n_epochs: -1 
  total_steps: 3355000
  gradient_clipping: 10.0
  gradient_accumulate_steps: 1
  fp16: true
  log_step: 200

task:
  _name: audio_pretraining
  data: /mnt/data/ycevan/datasets/LibriSpeech/manifest
  max_sample_size: 250000
  min_sample_size: 32000
  normalize: false

pretrain_expert:
  datarc:
    num_workers: 6
    train_batch_size: 12

prune:
  sparsity: [.2,.3,.4,.5,.55,.6,.65,.675,.7,.71,.72,.73,.74,.75,.76,.77,.78,.79,.8] #,.81,.82,.83,.84,.85,.86,.87,.88,.89,.9, .91, .915, .92, .925, .93, .935, .94, .945, .95]
  # warmup, and period are steps
  warmup: 5000 # before pruning
  period: [0, 125000, 200000, 500000, 700000, 900000, 1075000, 1300000, 1650000,  #.7
           1875000, 2075000, 2150000, 2275000, 2450000, 2600000, 2825000, 2950000, 3075000, 3225000] #.8
          #3350000, 3450000] # between two pruning
  n_iters: 19 #38 # times of pruning
  pruning_condition: "normal" # normal, converge
  converge_loss_tolerance: 0.001 # only work in converge mode, tolerant how much performance drop
  average_length: 15000 # only work in converge mode
  smooth_factor: 0.9998
  strategy: "L1Unstructured"

optimizer:
  lr: 1.e-5
  betas: [0.9, 0.999]
  eps: 1.e-8
  weight_decay: 0
